# üß† RAG Memory System - Documentation

## Vue d'ensemble

Le syst√®me RAG (Retrieval-Augmented Generation) permet √† Vera de se souvenir de **toutes** les conversations pass√©es et d'utiliser cette m√©moire pour enrichir ses r√©ponses futures.

## üéØ Fonctionnalit√©s

### 1. **M√©moire Longue Dur√©e**
- Stockage permanent de toutes les conversations dans Supabase (PostgreSQL)
- Chaque conversation est convertie en vecteur d'embedding (768 dimensions via Gemini)
- Index vectoriel `ivfflat` pour recherche ultra-rapide (cosine similarity)

### 2. **Recherche S√©mantique**
- Trouve automatiquement les 3 conversations les plus similaires √† la question actuelle
- Seuil de similarit√© : 75% minimum
- Pas de limite par utilisateur ‚Üí b√©n√©ficie de toute la base de connaissances

### 3. **Enrichissement Contextuel**
- Les conversations similaires sont inject√©es dans le contexte envoy√© √† Vera
- Vera peut r√©f√©rencer des discussions pass√©es pour donner des r√©ponses plus coh√©rentes
- √âvite de r√©p√©ter les m√™mes analyses

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     USER QUERY                                  ‚îÇ
‚îÇ                     "Cette vid√©o TikTok est-elle vraie?"        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  √âTAPE 1: G√âN√âRATION EMBEDDING                                  ‚îÇ
‚îÇ  embedding.service.js ‚Üí Gemini embedding-001                    ‚îÇ
‚îÇ  Input: "Cette vid√©o TikTok est-elle vraie?"                    ‚îÇ
‚îÇ  Output: [0.123, -0.456, 0.789, ...] (768 dimensions)           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  √âTAPE 2: RECHERCHE VECTORIELLE                                 ‚îÇ
‚îÇ  vector-store.service.js ‚Üí Supabase match_conversations()       ‚îÇ
‚îÇ  Search: cosine similarity > 0.75                               ‚îÇ
‚îÇ  Limit: 3 conversations                                         ‚îÇ
‚îÇ  Output:                                                        ‚îÇ
‚îÇ  [1] 89% similar: "TikTok deepfake d√©tection..."               ‚îÇ
‚îÇ  [2] 82% similar: "Vid√©o manipul√©e analyse..."                 ‚îÇ
‚îÇ  [3] 76% similar: "Fact-check TikTok..."                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  √âTAPE 3: CONSTRUCTION CONTEXTE                                 ‚îÇ
‚îÇ  vera.service.js ‚Üí checkContent()                               ‚îÇ
‚îÇ  Context: User question + RAG memory + conversation history     ‚îÇ
‚îÇ  Payload:                                                       ‚îÇ
‚îÇ  {                                                              ‚îÇ
‚îÇ    query: "Nouvelle question: ...\n                            ‚îÇ
‚îÇ            üíæ M√âMOIRE (conversations pass√©es):\n               ‚îÇ
‚îÇ            [1] 89% similar: Q: ... R: ..."                     ‚îÇ
‚îÇ  }                                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  √âTAPE 4: VERA API CALL                                         ‚îÇ
‚îÇ  Vera AI re√ßoit le contexte enrichi                             ‚îÇ
‚îÇ  Analyse avec outils + m√©moire                                  ‚îÇ
‚îÇ  Output: R√©ponse d√©taill√©e + verdict                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  √âTAPE 5: STOCKAGE                                              ‚îÇ
‚îÇ  1. Combiner question + r√©ponse                                 ‚îÇ
‚îÇ  2. G√©n√©rer embedding de la conversation compl√®te               ‚îÇ
‚îÇ  3. Stocker dans Supabase (conversations table)                 ‚îÇ
‚îÇ  4. Index automatique pour futures recherches                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üì¶ Stack Technique

### Backend (Node.js)
- **@supabase/supabase-js**: Client PostgreSQL + pgvector
- **@google/generative-ai**: Gemini embeddings (gratuit !)
- **embedding.service.js**: G√©n√©ration d'embeddings 768D
- **vector-store.service.js**: CRUD conversations + recherche vectorielle

### Database (Supabase)
- **PostgreSQL 15+** avec extension **pgvector**
- Table `conversations`:
  - `id` (UUID)
  - `user_id` (TEXT)
  - `user_query` (TEXT)
  - `vera_response` (TEXT)
  - `embedding` (vector(768)) ‚Üê Gemini embeddings
  - `metadata` (JSONB)
  - `created_at`, `updated_at` (TIMESTAMPTZ)
- Index `ivfflat` pour recherche vectorielle rapide
- Fonction RPC `match_conversations()` pour cosine similarity

### AI Models
- **Google Gemini embedding-001**: 768 dimensions, gratuit
- Alternative test√©e: OpenAI text-embedding-ada-002 (1536D, payant)

## üöÄ Setup

### 1. Configuration Supabase

1. Cr√©er un projet sur [supabase.com](https://supabase.com)
2. Aller dans **SQL Editor**
3. Ex√©cuter le script `database/supabase-rag-setup.sql`
4. V√©rifier que la table `conversations` est cr√©√©e avec l'index vectoriel

### 2. Variables d'environnement

Ajouter dans `.env`:

```env
# Supabase
SUPABASE_URL=https://xxx.supabase.co
SUPABASE_KEY=eyJxxxxxxxxxxxx  # anon public key
SUPABASE_SERVICE_KEY=eyJxxxxxxxxxxxx  # service_role (optionnel)

# Google Gemini
GEMINI_API_KEY=AIzaSyxxxxxxxxxxxx
```

### 3. Installation

```bash
cd vera-back
npm install @supabase/supabase-js @google/generative-ai
```

### 4. Test

```bash
# D√©marrer le serveur
npm run api

# Tester l'endpoint chat
curl -X POST http://localhost:3000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Cette vid√©o TikTok est-elle vraie?", "mediaUrls": ["https://tiktok.com/@user/video/123"]}'

# Rechercher des conversations similaires
curl -X POST http://localhost:3000/api/memory/search \
  -H "Content-Type: application/json" \
  -d '{"query": "deepfake", "limit": 5}'

# R√©cup√©rer l'historique
curl http://localhost:3000/api/memory/history/user-123
```

## üì° API Endpoints

### Chat avec RAG
```http
POST /api/chat
Content-Type: application/json

{
  "message": "Question √† poser",
  "conversationHistory": [...],  // Optionnel
  "mediaUrls": [...]              // Optionnel
}

Response:
{
  "response": "R√©ponse enrichie avec m√©moire RAG",
  "result": {...}
}
```

### Recherche S√©mantique
```http
POST /api/memory/search
Content-Type: application/json

{
  "query": "deepfake TikTok",
  "userId": "user-123",  // Optionnel (filtre)
  "limit": 5,            // D√©faut: 5
  "threshold": 0.7       // D√©faut: 0.7 (70% similarit√©)
}

Response:
{
  "success": true,
  "count": 3,
  "conversations": [
    {
      "id": "uuid",
      "user_query": "...",
      "vera_response": "...",
      "similarity": 0.89,
      "created_at": "2025-12-03T..."
    }
  ]
}
```

### Historique Utilisateur
```http
GET /api/memory/history/:userId?limit=50

Response:
{
  "success": true,
  "count": 42,
  "conversations": [...]
}
```

### Conversation Sp√©cifique
```http
GET /api/memory/conversation/:id

Response:
{
  "success": true,
  "conversation": {...}
}
```

### Nettoyage (Admin)
```http
POST /api/memory/cleanup
Content-Type: application/json

{
  "daysOld": 90  // Supprimer conversations > 90 jours
}

Response:
{
  "success": true,
  "deletedCount": 127
}
```

## üîß Maintenance

### Optimisation Index
Si la base grandit beaucoup (>100k conversations), reconstruire l'index:

```sql
-- Supabase SQL Editor
REINDEX INDEX idx_conversations_embedding;
```

### Statistiques
```sql
SELECT * FROM get_conversation_stats();
```

Retourne:
- Total conversations
- Utilisateurs uniques
- Date premi√®re/derni√®re conversation
- Longueur moyenne des r√©ponses

### Nettoyage Automatique
Configurer un cron job Supabase (Database > Cron Jobs):

```sql
-- Tous les jours √† 3h du matin, supprimer conversations > 90 jours
SELECT cron.schedule(
  'cleanup-old-conversations',
  '0 3 * * *',
  $$ SELECT cleanup_old_conversations(90); $$
);
```

## üí° Cas d'Usage

### 1. Coh√©rence Multi-Conversations
**Sans RAG:**
```
User: "Cette vid√©o TikTok de @influencer est vraie?"
Vera: "Oui, v√©rifi√©e."

[2 jours plus tard]
User: "Et la vid√©o de @influencer alors?"
Vera: "D√©sol√©, je ne sais pas de quoi tu parles."
```

**Avec RAG:**
```
User: "Et la vid√©o de @influencer alors?"
Vera: "üíæ Je me souviens ! Il y a 2 jours, nous avons analys√© sa vid√©o 
       et elle √©tait v√©rifi√©e comme authentique. Tu veux des d√©tails?"
```

### 2. Expertise Cumulative
Chaque analyse enrichit la base de connaissances. Si 100 utilisateurs posent des questions sur les deepfakes, le 101√®me b√©n√©ficie de toutes les analyses pr√©c√©dentes.

### 3. D√©tection de Patterns
Vera peut identifier des r√©currences:
```
"üí° J'ai remarqu√© 12 conversations similaires sur ce type de deepfake.
    La technique utilis√©e est connue et analys√©e en d√©tail ici: ..."
```

## üé® Int√©gration Frontend (TODO)

√Ä impl√©menter dans `vera-front`:

1. **Badge M√©moire**: Afficher "üß† 3 conversations similaires trouv√©es"
2. **Expandable Panel**: Cliquer pour voir les snippets des conversations pass√©es
3. **Timeline**: Vue chronologique de l'historique utilisateur
4. **Search Bar**: Recherche s√©mantique dans l'historique

## üîê S√©curit√©

### Row Level Security (RLS)
Actuellement d√©sactiv√© pour partager les connaissances entre utilisateurs.

Pour activer le RLS (chaque user voit uniquement ses conversations):
```sql
ALTER TABLE conversations ENABLE ROW LEVEL SECURITY;

CREATE POLICY "Users can view their own conversations"
  ON conversations FOR SELECT
  USING (auth.uid()::text = user_id);
```

### Rate Limiting
Impl√©menter un rate limit pour √©viter les abus:
```javascript
// api-server.js
const rateLimit = require('express-rate-limit');

const memorySearchLimiter = rateLimit({
  windowMs: 1 * 60 * 1000, // 1 minute
  max: 10 // 10 recherches max par minute
});

app.post('/api/memory/search', memorySearchLimiter, ...);
```

## üìä Performance

### Benchmarks (10k conversations)
- **G√©n√©ration embedding**: ~50ms (Gemini API)
- **Recherche vectorielle**: ~20ms (Supabase pgvector + ivfflat)
- **Stockage**: ~30ms
- **Total overhead RAG**: ~100ms

### Scalabilit√©
- **10k conversations**: Performance optimale
- **100k conversations**: Toujours rapide avec ivfflat
- **1M+ conversations**: Envisager sharding ou filtrage par date

## üêõ Troubleshooting

### Erreur "extension vector does not exist"
```sql
-- Supabase SQL Editor
CREATE EXTENSION IF NOT EXISTS vector;
```

### Erreur "function match_conversations does not exist"
R√©ex√©cuter le script `supabase-rag-setup.sql` complet.

### Embeddings dimension mismatch
Gemini = 768D, OpenAI = 1536D. Choisir un mod√®le et s'y tenir.

Pour changer de mod√®le:
```sql
-- Supabase SQL Editor
ALTER TABLE conversations ALTER COLUMN embedding TYPE vector(1536);
DROP INDEX idx_conversations_embedding;
CREATE INDEX idx_conversations_embedding ON conversations 
  USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);
```

### RAG non disponible (fallback gracieux)
Le syst√®me continue de fonctionner sans RAG si erreur. Check logs:
```
‚ö†Ô∏è RAG non disponible: SUPABASE_URL non d√©finie
```

## üöÄ √âvolutions Futures

### Phase 2: Multi-Modal RAG
- Stocker les embeddings d'images/vid√©os (CLIP, ImageBind)
- Recherche cross-modale: "Trouve toutes les vid√©os similaires √† cette image"

### Phase 3: RAG Contextuel
- Embeddings s√©par√©s pour: questions, r√©ponses, m√©tadonn√©es
- Recherche multi-vecteurs pour pr√©cision accrue

### Phase 4: RAG F√©d√©r√©
- Combiner m√©moire personnelle + base commune
- Privacy-preserving: chaque user garde son historique priv√©

## üìö R√©f√©rences

- [pgvector Documentation](https://github.com/pgvector/pgvector)
- [Gemini Embeddings](https://ai.google.dev/docs/embeddings_guide)
- [Supabase Vector Search](https://supabase.com/docs/guides/ai/vector-columns)
- [RAG Best Practices](https://www.pinecone.io/learn/retrieval-augmented-generation/)

---

**Cr√©√© le**: 3 D√©cembre 2025  
**Version**: 1.0.0  
**Auteur**: Vera AI Team  
**License**: MIT
